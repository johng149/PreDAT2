{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.checkpoint import load_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, model, _, _ = load_checkpoint(checkpoint_path=\"checkpoints/shakespeare.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Projects\\ML\\PreDAT2\\.venv\\lib\\site-packages\\transformers\\utils\\hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from src.tokenizer.model import Tokenizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer = Tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.shakespeare.dataset import Dataset as ShakespeareDataset\n",
    "from src.datasets.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"data/shakespeare\"\n",
    "max_seq_len = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = ShakespeareDataset(f\"{dataset_path}/test\", max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "min_ratio: int = 2\n",
    "max_ratio: int = 4\n",
    "max_num_spans: int = 6\n",
    "max_span_fill: float = 0.8\n",
    "min_num_spans: int = 0\n",
    "min_span_fill: float = 0\n",
    "hard_fill = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(\n",
    "    ds=test_ds,\n",
    "    batch_size=batch_size,\n",
    "    enc_span_idx=tokenizer.enc_span_token,\n",
    "    target_span_idx=tokenizer.targ_span_token,\n",
    "    fill_idx=tokenizer.mask_token,\n",
    "    eos_idx=tokenizer.eos_token,\n",
    "    bos_idx=tokenizer.bos_token,\n",
    "    min_ratio=min_ratio,\n",
    "    max_ratio=max_ratio,\n",
    "    max_num_spans=max_num_spans,\n",
    "    max_span_fill=max_span_fill,\n",
    "    min_num_spans=min_num_spans,\n",
    "    min_span_fill=min_span_fill,\n",
    "    hard_fill=hard_fill,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_single(transition, emission, lookahead=False):\n",
    "    vertex_count, vocab_size = emission.shape\n",
    "\n",
    "    if lookahead:\n",
    "        values, indices = emission.max(dim=1)\n",
    "        transition = transition + values.unsqueeze(0)\n",
    "\n",
    "    tokens = torch.argmax(emission, dim=1)\n",
    "    edges = torch.argmax(transition, dim=1)\n",
    "\n",
    "    edges[edges == 0] = vertex_count\n",
    "\n",
    "    i = 0\n",
    "    output = [tokens[i].item()]\n",
    "    while i < vertex_count:\n",
    "        i = edges[i].item()\n",
    "        if i >= vertex_count:\n",
    "            break\n",
    "        output.append(tokens[i].item())\n",
    "\n",
    "    return torch.tensor(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "(batch,\n",
    "enc,\n",
    "targ,\n",
    "dec_pos,\n",
    "dec_v,\n",
    "target_lens,\n",
    "vertex_lens,\n",
    "target_span_indices,\n",
    "ratio) = test_dl.get_batch()\n",
    "transition, emissions = model(\n",
    "    enc_x=enc,\n",
    "    dec_x_vocab=dec_v,\n",
    "    dec_x_pos=dec_pos,\n",
    "    vertex_lens=vertex_lens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = decode_single(transition[0], emissions[0], lookahead=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch raw: tensor([[  198, 43468,   415,    25,   198, 15597,   534,  3470,  8059,   284]])\n",
      "decoded batch: \n",
      "Pedant:\n",
      "Keep your hundred pounds to\n"
     ]
    }
   ],
   "source": [
    "print(f\"batch raw: {batch}\\ndecoded batch: {tokenizer.tokenizer.decode(batch[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc raw: tensor([[50256,   198, 43468, 50258,   198, 50258,  8059, 50258, 50256]])\n",
      "decoded enc: <|endoftext|>\n",
      "Ped\n",
      " pounds<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(f\"enc raw: {enc}\\ndecoded enc: {tokenizer.tokenizer.decode(enc[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target raw: tensor([[50256, 50259,   415,    25, 50259, 15597,   534,  3470, 50259,   284,\n",
      "         50256]])\n",
      "decoded target: <|endoftext|>ant:Keep your hundred to<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(f\"target raw: {targ}\\ndecoded target: {tokenizer.tokenizer.decode(targ[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded: <|endoftext|>,Is of of<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(f\"decoded: {tokenizer.tokenizer.decode(decoded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
