{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \"./\"))\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.wikipedia.dataset import Dataset as WikipediaDataset\n",
    "from transformers import AutoTokenizer\n",
    "from src.tokenizer.model import Tokenizer\n",
    "#from torch.utils.data import DataLoader\n",
    "from src.datasets.dataloader import DataLoader\n",
    "from src.training.checkpoint import load_checkpoint\n",
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "min_ratio: int = 3\n",
    "max_ratio: int = 3\n",
    "max_num_spans: int = 1\n",
    "max_span_fill: float = 0.15\n",
    "min_num_spans: int = 1\n",
    "min_span_fill: float = 0.15\n",
    "hard_fill = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../data/wikipedia\"\n",
    "max_seq_len = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = WikipediaDataset(f\"{dataset_path}/test\", max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer = Tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(\n",
    "    ds=test_ds,\n",
    "    batch_size=batch_size,\n",
    "    enc_span_idx=tokenizer.enc_span_token,\n",
    "    target_span_idx=tokenizer.targ_span_token,\n",
    "    fill_idx=tokenizer.mask_token,\n",
    "    eos_idx=tokenizer.eos_token,\n",
    "    bos_idx=tokenizer.bos_token,\n",
    "    min_ratio=min_ratio,\n",
    "    max_ratio=max_ratio,\n",
    "    max_num_spans=max_num_spans,\n",
    "    max_span_fill=max_span_fill,\n",
    "    min_num_spans=min_num_spans,\n",
    "    min_span_fill=min_span_fill,\n",
    "    hard_fill=hard_fill,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "checkpoint_path = \"../checkpoints\"\n",
    "checkpoint_name = \"wikipedia_one_span.pth\"\n",
    "writer_path = \"runs/wikipedia_one_span\"\n",
    "\n",
    "\n",
    "epoch, model, optimizer, writer = load_checkpoint(\n",
    "    checkpoint_path=f\"{checkpoint_path}/{checkpoint_name}\",\n",
    "    optim_class=None,\n",
    "    open_writer=False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode_single(transitions: Tensor, emissions: Tensor):\n",
    "    \"\"\"\n",
    "    Greedy traversal of DAG to decode sequence\n",
    "\n",
    "    @param transitions: shape (num_vertices, num_vertices)\n",
    "    @param emissions: shape (num_vertices, vocab_size)\n",
    "\n",
    "    @return: decoded sequence\n",
    "    \"\"\"\n",
    "    num_vertices, vocab_size = emissions.shape\n",
    "    tokens = emissions.argmax(dim=-1)\n",
    "    edges = transitions.argmax(dim=-1)\n",
    "    i = 0\n",
    "    output = []\n",
    "    while i < num_vertices:\n",
    "        output.append(tokens[i])\n",
    "        i = edges[i]\n",
    "        if i == 0:\n",
    "            break\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    batch,\n",
    "    enc,\n",
    "    targ,\n",
    "    dec_pos,\n",
    "    dec_v,\n",
    "    target_lens,\n",
    "    vertex_lens,\n",
    "    target_span_indices,\n",
    "    ratio,\n",
    ") = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probs, emission_probs = model(\n",
    "    enc_x=enc, dec_x_vocab=dec_v, dec_x_pos=dec_pos, vertex_lens=vertex_lens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  4,  4,  4,  6,  6,  7,  9, 45, 12, 25, 16, 13, 15, 16, 16, 19, 19,\n",
       "        47, 22, 36, 47, 25, 47, 47, 33, 36, 47, 47, 36, 47, 47, 36, 37, 45, 36,\n",
       "        47, 45, 48, 50, 47, 45, 47, 47, 45, 50, 48, 48, 50, 50,  0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_probs[0].argmax(dim=-1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = greedy_decode_single(transition_probs[0], emission_probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>K River River.\\n\\n\\n\\n\\nGes of Mas<|endoftext|>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenizer.decode(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>Beitbridge road.\\n\\nRivers of Zimbabwe\\nMas<|endoftext|>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenizer.decode(targ[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
