{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_seq = torch.tensor([1,3,2,4,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vertices = 10\n",
    "vocab_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = torch.tensor(\n",
    "    [\n",
    "        [0, 0.9, 0.04, 0, 0.06, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 0.5, 0.5, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    ]\n",
    ")\n",
    "emission_matrix = torch.ones(num_vertices, vocab_size) / vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs(target_seq, transition_matrix, emission_matrix, seq, prob, storage):\n",
    "    if len(seq) == len(target_seq):\n",
    "        storage.append((seq, prob))\n",
    "        return\n",
    "    if len(seq) == 0:\n",
    "        start_prob = emission_matrix[0][target_seq[0]]\n",
    "        dfs(target_seq, transition_matrix, emission_matrix, [0], start_prob, storage)\n",
    "    else:\n",
    "        next_candidates = transition_matrix[seq[-1]]\n",
    "        for i, p in enumerate(next_candidates):\n",
    "            if p > 0:\n",
    "                new_prob = prob * p * emission_matrix[i][target_seq[len(seq)]]\n",
    "                dfs(target_seq, transition_matrix, emission_matrix, seq + [i], new_prob, storage)\n",
    "                \n",
    "def max_prob_path(storage):\n",
    "    max_prob = 0\n",
    "    max_seq = []\n",
    "    for seq, prob in storage:\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            max_seq = seq\n",
    "    return max_seq, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs(target_seq, transition_matrix, emission_matrix, [], 1, storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_prob = sum([prob for seq, prob in storage])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 6, 7, 9], tensor(0.0003))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_prob_path(storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs_lynchpin(target_seq, transition_matrix, emission_matrix, seq, prob, storage, assignments=None):\n",
    "    if len(seq) == len(target_seq):\n",
    "        storage.append((seq, prob))\n",
    "        return\n",
    "    if len(seq) == 0:\n",
    "        start_prob = emission_matrix[0][target_seq[0]]\n",
    "        dfs_lynchpin(target_seq, transition_matrix, emission_matrix, [0], start_prob, storage, assignments)\n",
    "    else:\n",
    "        next_candidates = transition_matrix[seq[-1]]\n",
    "        current_assignment = -1 if assignments is None else assignments[len(seq)]\n",
    "        for i, p in enumerate(next_candidates):\n",
    "            if p > 0 and (current_assignment == -1 or current_assignment == i):\n",
    "                new_prob = prob * p * emission_matrix[i][target_seq[len(seq)]]\n",
    "                dfs_lynchpin(\n",
    "                    target_seq, \n",
    "                    transition_matrix, \n",
    "                    emission_matrix, \n",
    "                    seq + [i], \n",
    "                    new_prob, \n",
    "                    storage,\n",
    "                    assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments = torch.tensor([-1, 4, -1, 7, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_lynchpin(target_seq, transition_matrix, emission_matrix, [], 1, storage, assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([0, 4, 5, 7, 9], tensor(9.6000e-06)), ([0, 4, 6, 7, 9], tensor(9.6000e-06))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lynchpin_prob = sum([prob for seq, prob in storage])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_gather(vectors, indices):\n",
    "    \"\"\"\n",
    "    Gathers (batched) vectors according to indices.\n",
    "    Arguments:\n",
    "        vectors: Tensor[N, L, D]\n",
    "        indices: Tensor[N, K] or Tensor[N]\n",
    "    Returns:\n",
    "        Tensor[N, K, D] or Tensor[N, D]\n",
    "    \"\"\"\n",
    "    N, L, D = vectors.shape\n",
    "    squeeze = False\n",
    "    if indices.ndim == 1:\n",
    "        squeeze = True\n",
    "        indices = indices.unsqueeze(-1)\n",
    "    N2, K = indices.shape\n",
    "    assert N == N2\n",
    "    indices = einops.repeat(indices, \"N K -> N K D\", D=D)\n",
    "    out = torch.gather(vectors, dim=1, index=indices)\n",
    "    if squeeze:\n",
    "        out = out.squeeze(1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x: Tensor, dim: int) -> Tensor:\n",
    "    # Solving nan issue when x contains -inf\n",
    "    # See https://github.com/pytorch/pytorch/issues/31829\n",
    "    # https://github.com/thu-coai/DA-Transformer/blob/main/fs_plugins/custom_ops/dag_loss.py\n",
    "    m, _ = x.max(dim=dim, keepdim=True)\n",
    "    mask = m == -float(\"inf\")\n",
    "    m = m.detach()\n",
    "    s = (x - m.masked_fill_(mask, 0)).exp_().sum(dim=dim, keepdim=True)\n",
    "    return s.masked_fill_(mask, 1).log_() + m.masked_fill_(mask, -float(\"inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dag_loss_raw(targets, transition_matrix, emission_probs):\n",
    "    \"\"\"\n",
    "    Calculates the directed acyclic graph (DAG) loss given the targets, transition matrix, and emission probabilities.\n",
    "    It returns the dynamic programming table of which one of the entries is the DAG loss.\n",
    "\n",
    "    Args:\n",
    "        targets (torch.Tensor): The target sequence of shape (batch_size, m).\n",
    "        transition_matrix (torch.Tensor): The transition matrix of shape (batch_size, l, l).\n",
    "        emission_probs (torch.Tensor): The emission probabilities of shape (batch_size, l, vocab_size).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The DAG loss of shape (batch_size, m, l).\n",
    "    \"\"\"\n",
    "    batch_size, m = targets.shape\n",
    "    _, l, vocab_size = emission_probs.shape\n",
    "    dp = torch.ones((batch_size, m, l), device=transition_matrix.device)\n",
    "    dp[dp == 1] = -float(\"inf\")\n",
    "    initial_probs = torch.gather(\n",
    "        emission_probs, dim=2, index=targets[:, 0].unsqueeze(1).unsqueeze(2)\n",
    "    )\n",
    "    dp[:, 0, 0] = initial_probs.squeeze(2).squeeze(1)\n",
    "    # assumes that transition_matrix and emission_probs are already in log space\n",
    "    # also we need to tranpose emission_probs so it is vocab_size x l\n",
    "    # so the vector gather works\n",
    "    emission_probs = emission_probs.transpose(1, 2)\n",
    "    for i in range(1, m):\n",
    "        dp[:, i, :] = vector_gather(emission_probs, targets[:, i]) + (\n",
    "            (\n",
    "                logsumexp(\n",
    "                    dp[:, i - 1, :].unsqueeze(1).transpose(1, 2) + transition_matrix,\n",
    "                    dim=1,\n",
    "                )\n",
    "            ).squeeze(1)\n",
    "        )\n",
    "    return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = torch.log(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_matrix = torch.log(emission_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = transition_matrix.unsqueeze(0)\n",
    "emission_matrix = emission_matrix.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_seq = target_seq.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = dag_loss_raw(target_seq, transition_matrix, emission_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.0000e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 3.6000e-02, 1.6000e-03, 0.0000e+00, 2.4000e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2000e-04, 0.0000e+00,\n",
       "          2.4000e-04, 7.4400e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.5360e-03, 6.4000e-05, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2000e-04]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00032000005012378097"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(total_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003199999628122896"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(torch.exp(dp[0][-1][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_dp_slice_for_spans(\n",
    "    dp_slice: Tensor, target_span_indices: Tensor, iteration: int\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    @param dp: (batch_size, num_vertices, 1) tensor of scores that is being consiered\n",
    "        by the current iteration (position in target sequence)\n",
    "    @param target_span_indices: (batch_size, seq_len) tensor of indices, if element\n",
    "        is non-negative, it represents that position in target sequence span otherwise\n",
    "        it is negative\n",
    "    @param iteration: int, the current iteration in the target sequence\n",
    "\n",
    "    @return dp_slice: (batch_size, num_vertices, 1) tensor of scores that guarantees\n",
    "        that all paths considered go through the vertex specified by the target_span_indices\n",
    "        if it is non-negative, otherwise scores of paths are not changed to -inf\n",
    "    \"\"\"\n",
    "    res = dp_slice\n",
    "    res = res.squeeze(2)\n",
    "\n",
    "    # here we cannot simply use this res to update the dp table,\n",
    "    # because this res update assumes that all paths are allowed,\n",
    "    # however, if we are at a span, only the path that passes\n",
    "    # through the span's specified vertex is allowed\n",
    "    # to do this, we set the probability of all other vertices\n",
    "    # to -inf, so that they are not considered in the max operation\n",
    "    # in the next step. This guarantees in the next step, we only\n",
    "    # consider paths that pass through the span's specified vertex\n",
    "    curr_span_status = target_span_indices[:, iteration - 1]\n",
    "    in_span = curr_span_status >= 0\n",
    "\n",
    "    # we need to do a masked_fill here for curr_span_status, because\n",
    "    # we use gather to get the current probability of the span vertex\n",
    "    # in res, and gather doesn't support negative indices.\n",
    "    # this also means the corresponding probability might not be useful\n",
    "    # because if it is not a span then we don't enforce that paths\n",
    "    # must pass through the span's vertex. We account for this later\n",
    "    curr_span_status = curr_span_status.masked_fill(~in_span, 0).unsqueeze(1)\n",
    "\n",
    "    selection = res.gather(1, curr_span_status)\n",
    "\n",
    "    masked = torch.full_like(res, -float(\"inf\"))\n",
    "    masked.scatter_(1, curr_span_status, selection)\n",
    "\n",
    "    in_span = in_span.unsqueeze(-1)\n",
    "    in_span = in_span.expand_as(res)\n",
    "\n",
    "    res = torch.where(in_span, masked, res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dag_loss_raw_lynchpin(targets, transition_matrix, emission_probs, assignments):\n",
    "    \"\"\"\n",
    "    Calculates the directed acyclic graph (DAG) loss given the targets, transition matrix, and emission probabilities.\n",
    "    It returns the dynamic programming table of which one of the entries is the DAG loss.\n",
    "\n",
    "    Args:\n",
    "        targets (torch.Tensor): The target sequence of shape (batch_size, m).\n",
    "        transition_matrix (torch.Tensor): The transition matrix of shape (batch_size, l, l).\n",
    "        emission_probs (torch.Tensor): The emission probabilities of shape (batch_size, l, vocab_size).\n",
    "        assignments (torch.Tensor): The assignments of shape (batch_size, m).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The DAG loss of shape (batch_size, m, l).\n",
    "        \n",
    "    The assignments tensor is a tensor of shape (batch_size, m) where each element is an integer from 0 to l-1.\n",
    "    If the element is -1, then the corresponding element in the target sequence is free to be any of the l vertices.\n",
    "    Otherwise, at that position in the target sequence, the vertex must be the one specified by the assignment.\n",
    "    \"\"\"\n",
    "    batch_size, m = targets.shape\n",
    "    _, l, vocab_size = emission_probs.shape\n",
    "    dp = torch.ones((batch_size, m, l), device=transition_matrix.device)\n",
    "    dp[dp == 1] = -float(\"inf\")\n",
    "    initial_probs = torch.gather(\n",
    "        emission_probs, dim=2, index=targets[:, 0].unsqueeze(1).unsqueeze(2)\n",
    "    )\n",
    "    dp[:, 0, 0] = initial_probs.squeeze(2).squeeze(1)\n",
    "    # assumes that transition_matrix and emission_probs are already in log space\n",
    "    # also we need to tranpose emission_probs so it is vocab_size x l\n",
    "    # so the vector gather works\n",
    "    emission_probs = emission_probs.transpose(1, 2)\n",
    "    for i in range(1, m):\n",
    "        # before proceeding, we need to check the previous iteration's dp values\n",
    "        # to see if that it agrees with the assignments, if not, we need to adjust\n",
    "        # the dp values accordingly\n",
    "        prev_dp = dp[:, i - 1, :].unsqueeze(1).transpose(-1, -2)\n",
    "        prev_dp = correct_dp_slice_for_spans(prev_dp, assignments, i)\n",
    "        dp[:, i - 1, :] = prev_dp\n",
    "        dp[:, i, :] = vector_gather(emission_probs, targets[:, i]) + (\n",
    "            (\n",
    "                logsumexp(\n",
    "                    dp[:, i - 1, :].unsqueeze(1).transpose(1, 2) + transition_matrix,\n",
    "                    dim=1,\n",
    "                )\n",
    "            ).squeeze(1)\n",
    "        )\n",
    "    return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments = assignments.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp2 = dag_loss_raw_lynchpin(target_seq, transition_matrix, emission_matrix, assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -1.6094,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,\n",
       "              -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,     -inf,  -6.0323,     -inf,     -inf,\n",
       "              -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,     -inf,     -inf,  -8.3349,  -8.3349,\n",
       "              -inf,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,\n",
       "           -9.2512,     -inf,     -inf],\n",
       "         [    -inf,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,\n",
       "              -inf,     -inf, -10.8606]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.920000067912042e-05"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(total_lynchpin_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.920001523103565e-05"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(torch.exp(dp2[0][-1][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findPathBatched(\n",
    "    transition_matrix: Tensor,\n",
    "    emission_probs: Tensor,\n",
    "    target_seq: Tensor,\n",
    "    target_span_indices: Tensor,\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    @param transition_matrix: (b, n, n) tensor, `n` is number of vertices\n",
    "        the matrix should be in log-space, and describe an acyclic directed graph\n",
    "    @param emission_probs: (b, n, m) tensor, where `n` is the number of vertices\n",
    "        and `m` is the vocabulary size. The matrix should be in log-space\n",
    "    @param target_seq: (b, seq_len) tensor\n",
    "    @param target_span_indices: (b, seq_len) tensor, the indices of the target span\n",
    "        for their respective elements in `target_seq`. If the element in `target_seq`\n",
    "        is a span, then the corresponding element in `target_span_indices` should be\n",
    "        non-negative, otherwise it should be negative.\n",
    "\n",
    "    @return: (dp, backtrace) where `dp` is a (b, seq_len, n) tensor and `backtrace`\n",
    "        is a (b, seq_len - 1, n) tensor. `dp` contains the log-probabilities of the\n",
    "        most likely path to each vertex at each time step. `backtrace` contains\n",
    "        the backtrace pointers for each vertex at each time step.\n",
    "\n",
    "    Find the most likely path through the graph given the target sequence and\n",
    "    the transition and emission probabilities. The graph is an acyclic directed\n",
    "    graph, and the transition and emission probabilities are in log-space.\n",
    "\n",
    "    The path must start at the first vertex and end at the last vertex with\n",
    "    the number of vertices in the path equal to the length of the target sequence.\n",
    "    \"\"\"\n",
    "    b, n, m = emission_probs.shape\n",
    "    _, seq_len = target_seq.shape\n",
    "    dp = torch.zeros(b, seq_len, n, device=transition_matrix.device)\n",
    "    dp.fill_(-float(\"inf\"))\n",
    "    initial_probs = torch.gather(\n",
    "        emission_probs, dim=2, index=target_seq[:, 0].unsqueeze(1).unsqueeze(2)\n",
    "    )\n",
    "    dp[:, 0, 0] = initial_probs.squeeze(2).squeeze(1)\n",
    "    backtrace = torch.zeros(\n",
    "        b, seq_len - 1, n, dtype=torch.long, device=transition_matrix.device\n",
    "    )\n",
    "\n",
    "    emission_probs = emission_probs.transpose(-1, -2)\n",
    "    for i in range(1, seq_len):\n",
    "        currDp = dp[:, i - 1, :].unsqueeze(1).transpose(-1, -2)\n",
    "        currDp = correct_dp_slice_for_spans(currDp, target_span_indices, i)\n",
    "        dp[:, i - 1, :] = currDp\n",
    "        currDp = currDp.unsqueeze(2)\n",
    "        pathProbs = currDp + transition_matrix\n",
    "        max_v, max_idx = torch.max(pathProbs, dim=1)\n",
    "        res = max_v + vector_gather(emission_probs, target_seq[:, i])\n",
    "        dp[:, i, :] = res\n",
    "        backtrace[:, i - 1, :] = max_idx\n",
    "    return dp, backtrace\n",
    "\n",
    "\n",
    "def backtrace(\n",
    "    backtrace: Tensor,\n",
    "    target_lens: Tensor,\n",
    "    vertex_lens: Tensor,\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    @param backtrace: (batch_size, seq_len - 1, n) tensor\n",
    "    @param target_lens: (batch_size,) tensor\n",
    "    @param vertex_lens: (batch_size,) tensor\n",
    "    @param target_span_indices: (batch_size, seq_len) tensor\n",
    "    @param target_span_indices_pad_value: int\n",
    "\n",
    "    @return: (paths, is_padding) where `paths` is a (batch_size, seq_len) tensor\n",
    "        and `is_padding` is a (batch_size, seq_len) tensor. `paths` contains the\n",
    "        most likely path through the graph given the backtrace tensor. `is_padding`\n",
    "        is a boolean tensor that indicates whether the path is padding or not.\n",
    "        If the respective element in `is_padding` is True, then the path at that\n",
    "        position is padding.\n",
    "\n",
    "    Given the backtrace tensor, find the most likely path\n",
    "    that starts from the first vertex and ends at the last vertex.\n",
    "\n",
    "    For any padding vertices, we assume that the previous vertex\n",
    "    is where the path comes from\n",
    "    \"\"\"\n",
    "    b, t, v = backtrace.shape\n",
    "    offset_target_lens = target_lens - 1\n",
    "    start_vertex = vertex_lens - 1\n",
    "    backtrace = backtrace.transpose(-1, -2)\n",
    "    tracepath = torch.zeros(b, dtype=torch.long, device=backtrace.device) + v - 1\n",
    "    traces = []\n",
    "    for i in range(t - 1, -1, -1):\n",
    "        tracepath = torch.where((i + 1) == offset_target_lens, start_vertex, tracepath)\n",
    "        nextpath = vector_gather(backtrace, tracepath)[:, i]\n",
    "        nextpath = torch.where(nextpath == 0, tracepath - 1, nextpath)\n",
    "        tracepath = torch.where(i >= offset_target_lens, -1, tracepath)\n",
    "        traces.append(tracepath)\n",
    "        tracepath = nextpath\n",
    "    traces.append(tracepath * 0)\n",
    "    traces = torch.stack(traces[::-1], dim=-1)\n",
    "    return traces, traces < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_path, dp_bt = findPathBatched(transition_matrix, emission_matrix, target_seq, assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces, is_padding = backtrace(dp_bt, torch.tensor([5]), torch.tensor([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 4, 5, 7, 9]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
