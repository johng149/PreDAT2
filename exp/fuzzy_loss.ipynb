{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ngram_loss(probs, transition, tgt_tokens):\n",
    "    # probs: batch_size x num_vertices x vocab_size\n",
    "    # transition: batch_size x num_vertices x num_vertices\n",
    "    # tgt_tokens: batch_size x tgt_len\n",
    "    # we assume tgt_tokens have no padding (all the same length)\n",
    "\n",
    "    ngrams_order = 2\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tgt_tokens_list = tgt_tokens.tolist()\n",
    "        ngrams_dict_bsz = [{} for i in range(tgt_tokens.size(0))]\n",
    "        ngrams_list_bsz = [[] for i in range(tgt_tokens.size(0))]\n",
    "        ngrams_max_count_bsz = [[] for i in range(tgt_tokens.size(0))]\n",
    "        for i in range(0,tgt_tokens.size(1)-ngrams_order+1):\n",
    "            for j in range(len(ngrams_dict_bsz)):\n",
    "                key = tuple(tgt_tokens_list[j][i:i+ngrams_order])\n",
    "                if key in ngrams_dict_bsz[j].keys():\n",
    "                    ngrams_max_count_bsz[j][ngrams_dict_bsz[j][key]] = ngrams_max_count_bsz[j][ngrams_dict_bsz[j][key]] + 1\n",
    "                else:\n",
    "                    ngrams_dict_bsz[j][key] = len(ngrams_list_bsz[j])\n",
    "                    ngrams_list_bsz[j].append(tgt_tokens_list[j][i:i+ngrams_order])\n",
    "                    ngrams_max_count_bsz[j].append(1)\n",
    "\n",
    "        # padded_ngrams_num = max([len(ngrams_list) for ngrams_list in ngrams_list_bsz])\n",
    "        # padded_ngrams_template = []\n",
    "        # for i in range(ngrams_order):\n",
    "        #     padded_ngrams_template.append(1)\n",
    "\n",
    "        # for i in range(len(ngrams_list_bsz)):\n",
    "        #     while len(ngrams_list_bsz[i]) < padded_ngrams_num:\n",
    "        #         ngrams_list_bsz[i].append(padded_ngrams_template)\n",
    "        #         ngrams_max_count_bsz[i].append(0)\n",
    "\n",
    "        ngrams_tensor_bsz = torch.LongTensor(ngrams_list_bsz).to(tgt_tokens.device) #bsz, number of ngram, length of ngram\n",
    "        ngrams_max_count_bsz = torch.tensor(ngrams_max_count_bsz).to(tgt_tokens.device) #bsz, number of ngram\n",
    "        del ngrams_dict_bsz\n",
    "        del ngrams_list_bsz\n",
    "\n",
    "\n",
    "\n",
    "    arrival_prob = torch.ones(transition.size(0),1).to(transition)\n",
    "    for i in range(1,transition.size(-1)):\n",
    "        arrival_prob = torch.cat([arrival_prob, torch.mul(arrival_prob[:,0:i],transition[:,0:i,i]).sum(dim=-1).unsqueeze(-1)],dim=-1)\n",
    "\n",
    "\n",
    "    expected_length = arrival_prob.sum(dim=-1)\n",
    "    expected_tol_num_of_ngrams = arrival_prob.unsqueeze(1)\n",
    "\n",
    "    for i in range(ngrams_order-1):\n",
    "        expected_tol_num_of_ngrams= torch.bmm(expected_tol_num_of_ngrams,transition)\n",
    "\n",
    "\n",
    "    expected_tol_num_of_ngrams = expected_tol_num_of_ngrams.sum(dim=-1).sum(dim=-1)\n",
    "\n",
    "\n",
    "    first_word_in_each_gram = ngrams_tensor_bsz[:,:,0].unsqueeze(-1) #bsz, number of ngram, 1\n",
    "\n",
    "    #bsz, number of ngram, prelen\n",
    "    first_word_probs = torch.gather(input=probs.unsqueeze(1).expand(-1,first_word_in_each_gram.size(-2),-1,-1),dim=-1,index=first_word_in_each_gram.unsqueeze(2).expand(-1,-1,probs.size(-2),-1)).squeeze()\n",
    "\n",
    "\n",
    "    expected_matched_num_of_ngrams = torch.mul(arrival_prob.unsqueeze(1),first_word_probs)\n",
    "    del first_word_probs        \n",
    "\n",
    "    for i in range(1,ngrams_order):\n",
    "        target_at_this_word = ngrams_tensor_bsz[:,:,i].unsqueeze(-1) #bsz, number of ngram, 1\n",
    "\n",
    "        #bsz, number of ngram, prelen\n",
    "        word_probs = torch.gather(input=probs.unsqueeze(1).expand(-1,target_at_this_word.size(-2),-1,-1),dim=-1,index=target_at_this_word.unsqueeze(2).expand(-1,-1,probs.size(-2),-1)).squeeze(dim=-1)\n",
    "\n",
    "        expected_matched_num_of_ngrams = torch.mul(torch.bmm(expected_matched_num_of_ngrams,transition),word_probs)\n",
    "        del word_probs\n",
    "\n",
    "\n",
    "    expected_matched_num_of_ngrams = expected_matched_num_of_ngrams.sum(dim=-1)\n",
    "\n",
    "    cutted_expected_matched_num_of_ngrams = torch.min(expected_matched_num_of_ngrams, ngrams_max_count_bsz.to(expected_matched_num_of_ngrams)).sum(dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "    #ngrams_F_score = cutted_expected_matched_num_of_ngrams / (expected_tol_num_of_ngrams[-1] + (tgt_tokens.ne(1).sum(dim=-1) - ngrams_order + 1))\n",
    "    cutted_precision = cutted_expected_matched_num_of_ngrams / expected_tol_num_of_ngrams\n",
    "    #reverse_length_ratio = tgt_tokens.ne(1).sum(dim=-1) / expected_length    \n",
    "    #brief_penalty = torch.min(torch.ones_like(reverse_length_ratio),torch.exp(1.0-reverse_length_ratio))\n",
    "\n",
    "    loss = cutted_precision\n",
    "\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union, List\n",
    "from torch import Tensor\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(target_seqs, n, as_tensor=True) -> Union[Tuple[Tensor, Tensor], Tuple[List, List]]:\n",
    "    \"\"\"\n",
    "    Given a 2D tensor of target sequences, and n-gram order, calculate\n",
    "    which n-grams are present in the target sequences as well as the\n",
    "    number of times they occur. We assume that the target sequences\n",
    "    has no padding\n",
    "\n",
    "    @param target_seqs: 2D tensor of target sequences\n",
    "    @param n: n-gram order\n",
    "    @param as_tensor: whether to return information as tensors or lists\n",
    "\n",
    "    @return: n-grams present in the target sequences and their counts\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = target_seqs.shape\n",
    "    assert seq_len >= n, \"Sequence length must be greater than or equal to n-gram order\"\n",
    "    with torch.no_grad():\n",
    "        ngrams = []\n",
    "        counts = []\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            ngram_dict = defaultdict(int)\n",
    "            for i in range(seq_len - n + 1):\n",
    "                ngram = tuple(target_seqs[b, i:i+n].tolist())\n",
    "                ngram_dict[ngram] += 1\n",
    "\n",
    "            ngrams.append(list(ngram_dict.keys()))\n",
    "            counts.append(list(ngram_dict.values()))\n",
    "        \n",
    "        if as_tensor:\n",
    "            ngrams = torch.tensor(ngrams, dtype=torch.long)\n",
    "            counts = torch.tensor(counts, dtype=torch.long)\n",
    "        \n",
    "        return ngrams, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passing_probability(transitions: Tensor, return_log: bool = False) -> Tensor:\n",
    "    \"\"\"\n",
    "    Given a tensor of transition probabilities, calculate the probability\n",
    "    of passing through each vertex in the graph, we'll assume that transitions\n",
    "    is log-probabilities. If return_log is true, we return the passing\n",
    "    probabilities in log-space otherwise we return them in linear space\n",
    "\n",
    "    @param transitions: tensor of transition probabilities\n",
    "    @return: tensor of passing probabilities\n",
    "    \"\"\"\n",
    "    batch_size, num_vertices, _ = transitions.shape\n",
    "    \n",
    "    probs = torch.zeros(batch_size, num_vertices, device=transitions.device)\n",
    "    probs[:, 0] = 1.0\n",
    "    probs = torch.log(probs)\n",
    "\n",
    "    for i in range(1, num_vertices):\n",
    "        transition_column = transitions[:, :, i]\n",
    "        current_sum = torch.logsumexp(probs + transition_column, dim=-1)\n",
    "        probs[:, i] = current_sum\n",
    "    \n",
    "    if return_log:\n",
    "        return probs\n",
    "    else:\n",
    "        return torch.exp(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_bmm(log_A, log_B):\n",
    "    \"\"\"\n",
    "    Performs a batch matrix multiplication in log space.\n",
    "\n",
    "    Args:\n",
    "        log_A: A tensor of shape (b, m, n) representing log(A).\n",
    "        log_B: A tensor of shape (b, n, p) representing log(B).\n",
    "\n",
    "    Returns:\n",
    "        A tensor of shape (b, m, p) representing log(A @ B).\n",
    "    \"\"\"\n",
    "    b, m, n = log_A.shape\n",
    "    _, _, p = log_B.shape\n",
    "\n",
    "    # 1. Expand dimensions to align for element-wise addition (broadcast)\n",
    "    log_A_expanded = log_A.unsqueeze(3)  # Shape (b, m, n, 1)\n",
    "    log_B_expanded = log_B.unsqueeze(1)  # Shape (b, 1, n, p)\n",
    "\n",
    "    # 2. Perform addition in log-space for equivalent to product in linear space\n",
    "    log_product = log_A_expanded + log_B_expanded  # Shape (b, m, n, p)\n",
    "\n",
    "    # 3. LogSumExp over the `n` dimension (matrix multiplication reduction)\n",
    "    log_C = torch.logsumexp(log_product, dim=2)  # Shape (b, m, p)\n",
    "\n",
    "    return log_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ngram_loss2(probs, transition, tgt_tokens, ngrams_order=2):\n",
    "    # probs: batch_size x num_vertices x vocab_size\n",
    "    # transition: batch_size x num_vertices x num_vertices\n",
    "    # tgt_tokens: batch_size x tgt_len\n",
    "    # we assume tgt_tokens have no padding (all the same length)\n",
    "    ngrams_tensor_bsz, ngrams_max_count_bsz = find_ngrams(tgt_tokens, ngrams_order)\n",
    "\n",
    "    arrival_prob = passing_probability(torch.log(transition), return_log=False)\n",
    "\n",
    "    expected_tol_num_of_ngrams = arrival_prob.unsqueeze(1)\n",
    "\n",
    "    for i in range(ngrams_order-1):\n",
    "        #expected_tol_num_of_ngrams= torch.bmm(expected_tol_num_of_ngrams,transition)\n",
    "        expected_tol_num_of_ngrams = log_bmm(expected_tol_num_of_ngrams.log(), transition.log()).exp()\n",
    "\n",
    "\n",
    "    expected_tol_num_of_ngrams = torch.log(expected_tol_num_of_ngrams)\n",
    "    # expected_tol_num_of_ngrams = expected_tol_num_of_ngrams.sum(dim=-1).sum(dim=-1)\n",
    "    expected_tol_num_of_ngrams = torch.logsumexp(expected_tol_num_of_ngrams, dim=-1)\n",
    "    expected_tol_num_of_ngrams = torch.logsumexp(expected_tol_num_of_ngrams, dim=-1)\n",
    "    expected_tol_num_of_ngrams = expected_tol_num_of_ngrams.exp()\n",
    "\n",
    "\n",
    "    first_word_in_each_gram = ngrams_tensor_bsz[:,:,0].unsqueeze(-1) #bsz, number of ngram, 1\n",
    "\n",
    "    #bsz, number of ngram, prelen\n",
    "    first_word_probs = torch.gather(input=probs.unsqueeze(1).expand(-1,first_word_in_each_gram.size(-2),-1,-1),dim=-1,index=first_word_in_each_gram.unsqueeze(2).expand(-1,-1,probs.size(-2),-1)).squeeze()\n",
    "\n",
    "\n",
    "    #expected_matched_num_of_ngrams = torch.mul(arrival_prob.unsqueeze(1),first_word_probs)\n",
    "    expected_matched_num_of_ngrams = arrival_prob.unsqueeze(1).log() + first_word_probs.log()\n",
    "    expected_matched_num_of_ngrams = expected_matched_num_of_ngrams.exp()\n",
    "    del first_word_probs        \n",
    "\n",
    "    for i in range(1,ngrams_order):\n",
    "        target_at_this_word = ngrams_tensor_bsz[:,:,i].unsqueeze(-1) #bsz, number of ngram, 1\n",
    "\n",
    "        #bsz, number of ngram, prelen\n",
    "        word_probs = torch.gather(input=probs.unsqueeze(1).expand(-1,target_at_this_word.size(-2),-1,-1),dim=-1,index=target_at_this_word.unsqueeze(2).expand(-1,-1,probs.size(-2),-1)).squeeze(dim=-1)\n",
    "\n",
    "        expected_matched_num_of_ngrams = expected_matched_num_of_ngrams.log()\n",
    "        #expected_matched_num_of_ngrams = torch.mul(torch.bmm(expected_matched_num_of_ngrams,transition),word_probs)\n",
    "        expected_matched_num_of_ngrams = log_bmm(expected_matched_num_of_ngrams, transition.log())\n",
    "        expected_matched_num_of_ngrams = expected_matched_num_of_ngrams + word_probs.log()\n",
    "        expected_matched_num_of_ngrams = expected_matched_num_of_ngrams.exp()\n",
    "        del word_probs\n",
    "\n",
    "\n",
    "    expected_matched_num_of_ngrams = expected_matched_num_of_ngrams.log()\n",
    "    #expected_matched_num_of_ngrams = expected_matched_num_of_ngrams.sum(dim=-1)\n",
    "    expected_matched_num_of_ngrams = torch.logsumexp(expected_matched_num_of_ngrams, dim=-1)\n",
    "    expected_matched_num_of_ngrams = expected_matched_num_of_ngrams.exp()\n",
    "\n",
    "    expected_matched_num_of_ngrams = expected_matched_num_of_ngrams.log()\n",
    "    ngrams_max_count_bsz = ngrams_max_count_bsz.log()\n",
    "    cutted_expected_matched_num_of_ngrams = torch.min(expected_matched_num_of_ngrams, ngrams_max_count_bsz)#.sum(dim=-1)\n",
    "    cutted_expected_matched_num_of_ngrams = torch.logsumexp(cutted_expected_matched_num_of_ngrams, dim=-1)\n",
    "    cutted_expected_matched_num_of_ngrams = cutted_expected_matched_num_of_ngrams.exp()\n",
    "\n",
    "\n",
    "    cutted_expected_matched_num_of_ngrams = cutted_expected_matched_num_of_ngrams.log()\n",
    "    expected_tol_num_of_ngrams = expected_tol_num_of_ngrams.log()\n",
    "    #ngrams_F_score = cutted_expected_matched_num_of_ngrams / (expected_tol_num_of_ngrams[-1] + (tgt_tokens.ne(1).sum(dim=-1) - ngrams_order + 1))\n",
    "    cutted_precision = cutted_expected_matched_num_of_ngrams - expected_tol_num_of_ngrams\n",
    "    #reverse_length_ratio = tgt_tokens.ne(1).sum(dim=-1) / expected_length    \n",
    "    #brief_penalty = torch.min(torch.ones_like(reverse_length_ratio),torch.exp(1.0-reverse_length_ratio))\n",
    "\n",
    "    loss = cutted_precision.exp()\n",
    "\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Case 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions = torch.tensor([\n",
    "    # these are not valid probabilities, but it is just for testing\n",
    "    [\n",
    "        [0.1, 0.2, 0.3, 0.4],\n",
    "        [0.5, 0.6, 0.7, 0.8],\n",
    "        [-0.1, -0.2, -0.3, -0.4],\n",
    "        [-0.5, -0.6, -0.7, -0.8],\n",
    "        [1.0, 0.0, -1.0, 0.9]\n",
    "    ],\n",
    "    [\n",
    "        [2.1, 2.2, 2.3, 2.4],\n",
    "        [2.5, 2.6, 2.7, 2.8],\n",
    "        [-2.1, -2.2, -2.3, -2.4],\n",
    "        [-2.5, -2.6, -2.7, -2.8],\n",
    "        [2.0, 3.0, -2.0, 2.9]\n",
    "    ]\n",
    "])\n",
    "batch_size, num_vertices, vocab_size = emissions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = torch.randn(batch_size, num_vertices, num_vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tril(torch.ones(num_vertices, num_vertices), diagonal=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions[mask.expand_as(transitions) == 1] = float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = torch.softmax(transitions, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions[transitions.isnan()] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions = torch.abs(emissions)\n",
    "transitions = torch.abs(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions = torch.softmax(emissions, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_tokens = torch.tensor([\n",
    "    [0, 3, 1],\n",
    "    [0, 2, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = compute_ngram_loss(emissions, transitions, tgt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ours = compute_ngram_loss2(emissions, transitions, tgt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1139, -0.1227]), tensor([-0.0315, -0.0327]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref, ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
