{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \"./\"))\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.wikipedia.dataset import Dataset as WikipediaDataset\n",
    "from transformers import AutoTokenizer\n",
    "from src.tokenizer.model import Tokenizer\n",
    "#from torch.utils.data import DataLoader\n",
    "from src.datasets.dataloader import DataLoader\n",
    "from src.training.checkpoint import load_checkpoint\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from src.nn.models.transformer import Transformer\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(\"wikipedia_cuda_dist_fsdp.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "min_ratio: int = 3\n",
    "max_ratio: int = 3\n",
    "max_num_spans: int = 1\n",
    "max_span_fill: float = 0.15\n",
    "min_num_spans: int = 1\n",
    "min_span_fill: float = 0.15\n",
    "hard_fill = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../data/wikipedia\"\n",
    "max_seq_len = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = WikipediaDataset(f\"{dataset_path}/test\", max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer = Tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(\n",
    "    ds=test_ds,\n",
    "    batch_size=batch_size,\n",
    "    enc_span_idx=tokenizer.enc_span_token,\n",
    "    target_span_idx=tokenizer.targ_span_token,\n",
    "    fill_idx=tokenizer.mask_token,\n",
    "    eos_idx=tokenizer.eos_token,\n",
    "    bos_idx=tokenizer.bos_token,\n",
    "    min_ratio=min_ratio,\n",
    "    max_ratio=max_ratio,\n",
    "    max_num_spans=max_num_spans,\n",
    "    max_span_fill=max_span_fill,\n",
    "    min_num_spans=min_num_spans,\n",
    "    min_span_fill=min_span_fill,\n",
    "    hard_fill=hard_fill,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = data[\"kwargs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(**kwargs)\n",
    "state_dict = load_file(\"model.safetensors\")\n",
    "model.load_state_dict(state_dict)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode_single(transitions: Tensor, emissions: Tensor):\n",
    "    \"\"\"\n",
    "    Greedy traversal of DAG to decode sequence\n",
    "\n",
    "    @param transitions: shape (num_vertices, num_vertices)\n",
    "    @param emissions: shape (num_vertices, vocab_size)\n",
    "\n",
    "    @return: decoded sequence\n",
    "    \"\"\"\n",
    "    num_vertices, vocab_size = emissions.shape\n",
    "    tokens = emissions.argmax(dim=-1)\n",
    "    edges = transitions.argmax(dim=-1)\n",
    "    i = 0\n",
    "    output = []\n",
    "    while i < num_vertices:\n",
    "        output.append(tokens[i])\n",
    "        i = edges[i]\n",
    "        if i == 0:\n",
    "            break\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    batch,\n",
    "    enc,\n",
    "    targ,\n",
    "    dec_pos,\n",
    "    dec_v,\n",
    "    target_lens,\n",
    "    vertex_lens,\n",
    "    target_span_indices,\n",
    "    ratio,\n",
    ") = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probs, emission_probs = model(\n",
    "    enc_x=enc, dec_x_vocab=dec_v, dec_x_pos=dec_pos, vertex_lens=vertex_lens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  4,  4,  4,  5,  6,  7,  8,  9, 45, 45, 45, 45, 45, 45, 45, 45, 45,\n",
       "        45, 45, 45, 45, 45, 45, 45, 45, 28, 45, 38, 45, 45, 45, 45, 38, 45, 45,\n",
       "        45, 45, 45, 45, 45, 45, 45, 45, 45, 53, 53, 53, 53, 53, 53, 53, 53,  0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_probs[0].argmax(dim=-1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = greedy_decode_single(transition_probs[0], emission_probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenizer.decode(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|> of the Réseau des Liaisons Aériennes Franca<|endoftext|>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenizer.decode(targ[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
